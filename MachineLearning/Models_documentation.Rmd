---
title: "**Avalanche prediction documentation**"
# Created by @Krystof Dytrt,dytrtk@fzp.czu.cz
# Data source @Valerian.Spusta, @CHMI
# text edited by @Marketa.Souckova
# For avalanche paper: Souckova et al. 2022

output: 
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    df_print: "kable"
  theme: lumen
---
<style>
body {
  font-family: Arial;
  font-size: 18px; 
}
pre {
  color: #708090;
}
p.caption {
  font-size: 13px;
}

</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width = 8.65, fig.align = 'right', message = FALSE, warning = FALSE)
```

# **General information**
This script provides information about the avalanche predictors driving the avalanche release and the differences between two types of avalanches: wet snow and slab avalanches. The two datasets were used for modeling the predictors using machine learning algorithms to obtain insights into what effects play a major role between two different types of avalanches.


## **libraries used for modelling**
```{r}
library(tidyverse)
library(DataExplorer)
library(naniar)
library(lubridate)
library(dendextend)
library(ggbiplot)
library(randomForest)
library(rpart)
library(rpart.plot)
library(tidymodels)
library(ModelMetrics)
library(ROSE)
library(vip)
library(plotly)
library(PCAtools)
```
## **theme plot definition**
```{r}
theme_set(theme_bw() + theme(plot.background = element_rect(fill = "white"),
                             panel.grid.major = element_line(colour = "#C0C0C0"),
                             legend.background = element_rect(colour = "black", fill = "white", size = 0.1),
                             plot.title = element_text(family = "Arial", face = "bold", size = 15),
                             axis.title = element_text(family = "Arial", face = "italic", size = 13.5)
                            )
          ) 
```


# **Datasets and data structure**

```{r}
## Import 
filename_A <- file.choose()
adcast_A <- readRDS(filename_A)

filename_B <- file.choose()
adcast_C <- readRDS(filename_B)


## data structure
str(adcast_A)
str(adcast_C)

adcast_A$event <- as.factor(adcast_A$event) 
adcast_C$event <- as.factor(adcast_C$event) 

rownames(adcast_A) <- 1:nrow(adcast_A)
rownames(adcast_C) <- 1:nrow(adcast_C)

summary(adcast_A)
summary(adcast_C)


## select all columns except "6" & SWE
adcast_A <- adcast_A %>%
  select(!matches("6") & !c("SWE_value", "SWE_value3", "stat", "CAW"))

adcast_C <- adcast_C %>%
  select(!matches("6") & !c("SWE_value", "SWE_value3", "stat", "CAW"))


## Na values: provides an at-a-glance ggplot of the missingness inside a dataframe, colouring cells #according to missingness, where black indicates a missing cell and grey indicates a present cell. 
vis_miss(adcast_A)
vis_miss(adcast_C)

adcast_A <- adcast_A[complete.cases(adcast_A),]
adcast_C <- adcast_C[complete.cases(adcast_C),]


## Filter date
adcast_A <- adcast_A %>%
  filter(Date >= 1979) 

adcast_C <- adcast_C %>%
  filter(Date >= 1979) 

table(adcast_A$event)
table(adcast_C$event)
```

Datasets should not contain any null values. Therefore we excluded them and filtered out observations that started from 1979. The reason for filtering is that we can obtain cleaner datasets. SWE was measured once a week, so it could be biased in the following sections and the variables for six days before the avalanche event. Changing variables for modeling is possible to get even more accurate results. The following modeling sections aim to obtain reliable predictive models that can be altered by the variables.

* input data are needed to be passed as the argument for file.choose function as we trained models on premise. (please specify the datasets by providing the file path before running the code, if you do not provide data input, only code will be displayed in this report)


# **Unsupervised Learning**
```{r}
## types of avalanches
both_types_aval <- inner_join(adcast_A %>%
          filter(event == 1), adcast_C %>%
                              filter(event == 1), by = "Date")
both_types_aval


for (i in names(adcast_A)) {
  x_i_a <- ggplot(adcast_A, aes_string(x = i, fill = "event")) + 
    geom_boxplot() 
  
  print(x_i_a)
}

for (i in names(adcast_C)) {
  x_i_c <- ggplot(adcast_C, aes_string(x = i, fill = "event")) + 
    geom_boxplot() 
  
  print(x_i_c)
}

```

plots of all the variables are crucial for understanding the data's statistical distribution. We checked the differences between the two groups we would like to predict.


## **Hclust**
```{r}
### adcast_A
Hc_a <- adcast_A[ ,3:ncol(adcast_A)] %>% 
  cor(use = "pairwise.complete.obs")
dist_Hc <- as.dist(1 - Hc_a)
hctree_A <- hclust(dist_Hc, method = "complete")
plot(hctree_A)

ggplot(hctree_A$height %>%
         as_tibble() %>%
         add_column(groups = length(hctree_A$height):1),
       aes(x = groups, y = value)) +
  geom_point() +
  geom_line()

plot(color_branches(hctree_A, k = 8))


### adcast_C
Hc_c <- adcast_C[ ,3:ncol(adcast_C)] %>% 
  cor(use = "pairwise.complete.obs")
dist_Hc_c <- as.dist(1 - Hc_c)
hctree_C <- hclust(dist_Hc_c, method = "complete")
plot(hctree_C)

ggplot(hctree_C$height %>%
         as_tibble() %>%
         add_column(groups = length(hctree_C$height):1),
       aes(x = groups, y = value)) +
  geom_point() +
  geom_line()

plot(color_branches(hctree_C, k = 11))
```

Hclustering was created to obtain information about the distance of correlation variables. Screeplots were conducted to sort main clusters by colors. 

## **PCA**
```{r}
### adcast_A
#### screeplot pca
Pca_s_A <- prcomp(adcast_A[, 3:ncol(adcast_A)], scale = TRUE, center = TRUE)

pve_s_A <- Pca_s_A$sdev^2 / sum(Pca_s_A$sdev^2)

plot(pve_s_A, xlab = "Principal Component",
     ylab = "proportion of variance",
     ylim = c(0, 1), type = "b")
```

Screeplot for PCA shows the variance for all the components. This is the screeplot for dataset A. screeplot for Dataset C will be part of the block code as we explained the key information here.


```{r,  message = FALSE, echo = FALSE}
#### pca: pca plots must be inserted into console for displaying
#ggplotly

ggbiplot(prcomp(adcast_A[, 3:ncol(adcast_A)], scale = TRUE, center = TRUE), 
         ellipse = TRUE,
         groups = adcast_A$event, 
         alpha = 0.1,
         varname.size = 5, 
         labels.size = 8,
         varname.adjust = 2, 
         varname.abbrev = FALSE
         ) +
  geom_point(aes(col = adcast_A$event), alpha = 0.1, size = 3)+
  scale_x_continuous(limits = c(-3, 3)) +
  scale_y_continuous(limits = c(-2, 2))


### adcast_C
#### screeplot pca
Pca_s_C <- prcomp(adcast_C[, 3:ncol(adcast_C)], scale = TRUE, center = TRUE)

pve_s_C <- Pca_s_C$sdev^2 / sum(Pca_s_C$sdev^2)

plot(pve_s_C, xlab = "Principal Component",
     ylab = "proportion of variance",
     ylim = c(0, 1), type = "b")


#### pca

ggbiplot(prcomp(adcast_C[, 3:ncol(adcast_C)], scale = TRUE, center = TRUE), 
         ellipse = TRUE,
         groups = adcast_C$event, 
         alpha = 0.1, 
         varname.size = 5, 
         labels.size = 4, 
         varname.adjust = 1, 
         varname.abbrev = FALSE
         ) +
  scale_x_continuous(limits = c(-2, 2)) +
  scale_y_continuous(limits = c(-4, 2))


### both avalanche types pca 
av_types <- rbind(
                  adcast_A %>%
                    filter(event == 1) %>%
                    mutate(Type = "slab"),
                  adcast_C %>%
                    filter(event == 1) %>%
                    mutate(Type = "wet")
                  ) 

ggbiplot(prcomp(av_types[, 3:ncol(subset(av_types, select = - Type))], scale = TRUE, center = TRUE), 
         ellipse = TRUE,
         groups = av_types$Type,
         alpha = 0.5, 
         varname.size = 5, 
         labels.size = 4, 
         varname.adjust = 1, 
         varname.abbrev = FALSE
         ) +
  geom_point(aes(col = av_types$Type), size = 6, alpha = 0.1)



##### for article adcast C -------------------------
ggbiplot(prcomp(adcast_C[, 3:ncol(adcast_C)] %>%  select(!matches("3") & !c("Rain_Tw_value", "SDdif4", "WSavg_value", "WD_value")), scale = TRUE, center = TRUE), 
                   ellipse = TRUE, 
                   groups = adcast_C$event, 
                   alpha = 0.1, 
                   varname.size = 6, 
                   labels.size = 5, 
                   varname.adjust = 1.5, 
                   varname.abbrev = FALSE
                  ) +
  scale_x_continuous(limits = c(-5, 4.5)) + 
  scale_y_continuous(limits = c(-5, 3))
```

PCA suggests the main differences in data for both datasets. Both datasets are overpopulated with no avalanche events. The dominance is evident, and the avalanche events tend to be hidden in the PCA even though the main characteristics of avalanche events in some way persist. The population of avalanche events tends to play some role in specific directions, but it is hard to distinguish the primary deviations for both populations due to low numbers of avalanche events.
The differences between both types of avalanches are comparable due to better-balanced data input. More data would be a beneficial factor in better understanding the avalanche trigger key characteristics.


# **Test of the model fit**
```{r}
randomForest(formula = event ~ ., data = adcast_A)
randomForest(formula = event ~ ., data = adcast_C)
```
The random forest classifier has a low error rate but cannot accurately predict the avalanche event. 
RF predicts well the non-avalanche events but is unable to reflect the avalanches. This is mainly caused because of the unbalanced dataset that we provided for training the algorithm. Bootstrap is the part of the RF compares, in most cases, the non-avalanche events with each other so the algorithm can predict when there is no avalanche. The avalanche information is hidden and cannot be predicted because it is not, in most cases, the part of the bootstrap selection, so the algorithm never classifies the avalanche events (or when it does, it is in a few insignificant cases). Thus we cannot predict avalanche events because it rarely selects the avalanche events for the decision trees before the bootstrap. 
For better understanding, we do not compare non-avalanche events with avalanche events on decision trees because we select high probability events with no avalanche events from the bag. Therefore, we have low out of bag error and never accurately predict avalanche events.

A solution to this is to create a balanced dataset or weighted decision trees. 

# **Datasets balancing**
Even though we will construct some supervised models in this part of the section, we are in a section called datasets balancing, which is prior to the final modeling. All the supervised techniques in this chapter are only for demonstration purposes, and the supervised learning chapter goes after.

Two main techniques will be used to balance dataset.

## **dataset upscale**
```{r}
## dataset upscale
### adcast_A
table(adcast_A$event)

adcast_A_aval <- adcast_A %>%
  filter(event == 1) %>% 
  slice(rep(1:n(), each = 14))

adcast_A_ups <- rbind(adcast_A_aval, adcast_A %>%
        filter(event == 0))

adcast_A_ups <- subset(adcast_A_ups, select = - Date)


#### Train test sets adcast_A
set.seed(1234)
split_node_A <- initial_split(adcast_A_ups, prop = 0.75, strata = event)
train_A <- training(split_node_A)
test_A <- testing(split_node_A)


### adcast_C
table(adcast_C$event)

adcast_C_aval <- adcast_C %>%
  filter(event == 1) %>% 
  slice(rep(1:n(), each = 45))

adcast_C_ups <- rbind(adcast_C_aval, adcast_C %>%
                        filter(event == 0))

adcast_C_ups <- subset(adcast_C_ups, select = - Date)


#### Train test sets adcast_C
set.seed(1234)
#proportion train 0.75, test set = 0.25â˜»
split_node_C <- initial_split(adcast_C_ups, prop = 0.75, strata = event)
train_C <- training(split_node_C)
test_C <- testing(split_node_C)
```

We obtained two datasets upscaled based on the input data.

```{r}
## DT evaulation
### general tree
#### adcast_A
frame_dt <- decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("classification")

set.seed(12)
dt_A_gen <- frame_dt %>%
  fit(formula = event ~ ., data = adcast_A_ups)
# plot DT 
rpart.plot(dt_A_gen$fit)


#### adcast_C
set.seed(12)
dt_C_gen <- frame_dt %>%
  fit(formula = event ~ ., data = adcast_C_ups, control = control_parsnip(5))


dt_C_gen <- rpart(formula = event ~ ., data = adcast_C_ups,  method = "class", control = list(maxdepth = 5))
# plot DT
rpart.plot(dt_C_gen)


### DT function train & test set
dt_all <- function(model_frame, trainset, testset) {

  model <- model_frame %>%
    fit(formula = event ~ ., data = trainset)
  
  dt_pred_c <- predict(model, new_data = testset)
  dt_pred_p <- predict(model, new_data = testset, type = "prob")
  
  pred_comb_c <- dt_pred_c %>%
    mutate(true_class = testset$event)
  
  dt_cm <- conf_mat(data = pred_comb_c,
                      estimate = .pred_class,
                      truth = true_class)
  
  pred_comb_p <- dt_pred_p %>%
    mutate(true_class = testset$event)
  
  dt_roc <- roc_curve(pred_comb_p,
            estimate = .pred_0,
            truth = true_class) %>%
    autoplot()
  
  dt_auc <- roc_auc(pred_comb_p,
                      estimate = .pred_0,
                      truth = true_class)
  
  tree_plot <- rpart.plot(model$fit)
  all <- list(dt_cm, dt_auc, dt_roc, tree_plot)
  
  return(all)
  
}


DT_A <- dt_all(frame_dt, train_A, test_A)
DT_A

DT_C <- dt_all(frame_dt, train_C, test_C)
DT_C

```

### DT assessment
We had problems with evaluating our model because of the repeated observations for the testing process. If we generated stratification datasets on the event before splitting, we could upscale our datasets and better use them for evaluation. 

```{r}
### RF with upscaled datasets without stratification before splitting
randomForest(formula = event ~ ., data = adcast_A_ups)
randomForest(formula = event ~ ., data = adcast_C_ups)
```

An upscaled dataset is better than using unbalanced datasets for DT and RF. The error rate decreased, and RF does not have the problem distinguishing between the two groups of events, and it seems to predict the avalanche event is highly accurate. However, this problem is that we have used a few observations to train and test the data. We used the same test data for the training (thanks to the upscale, we are pretty sure the same data went to train and the same for the test). That also means we predict on observations that are known for our model. If we wanted to get accurate results, we would have to split the train test set with strata on the event, then train and test our results and compare the model performance. The problem with this approach is that we do not have much data for splitting (especially for dataset_C).

```{r}
### Upscale with stratified observations for both train and test sets
#### adcast_A
table(adcast_A$event)

#### Train test sets adcast_A
set.seed(1234)
split_node_AA <- initial_split(subset(adcast_A, select = - Date), prop = 0.75, strata = event)
train_AA <- training(split_node_AA)
test_AA <- testing(split_node_AA)


##### train
adcast_AA_aval_train <- train_AA %>%
  filter(event == 1) %>% 
  slice(rep(1:n(), each = 15))

train_adcast_AA_ups <- rbind(adcast_AA_aval_train, train_AA %>%
        filter(event == 0))

##### test
adcast_AA_aval_test <- test_AA %>%
  filter(event == 1) %>% 
  slice(rep(1:n(), each = 15))

test_adcast_AA_ups <- rbind(adcast_AA_aval_test, test_AA %>%
        filter(event == 0))


#### adcast_C
table(adcast_C$event)

#### Train test sets adcast_A
set.seed(1234)
split_node_CC <- initial_split(subset(adcast_C, select = - Date), prop = 0.75, strata = event)
train_CC <- training(split_node_CC)
test_CC <- testing(split_node_CC)


##### train
adcast_CC_aval_train <- train_CC %>%
  filter(event == 1) %>% 
  slice(rep(1:n(), each = 30))

train_adcast_CC_ups <- rbind(adcast_CC_aval_train, train_CC %>%
        filter(event == 0))

##### test
adcast_CC_aval_test <- test_CC %>%
  filter(event == 1) %>% 
  slice(rep(1:n(), each = 30))

test_adcast_CC_ups <- rbind(adcast_CC_aval_test, test_CC %>%
        filter(event == 0))


#### DT 
DT_AA <- dt_all(frame_dt, train_adcast_AA_ups, test_adcast_AA_ups)
DT_AA

DT_CC <- dt_all(frame_dt, train_adcast_CC_ups, test_adcast_CC_ups)
DT_CC


#### RF A
RF_AA <- randomForest(formula = event ~ ., data = train_adcast_AA_ups)

pred_AA <- predict(object = RF_AA, newdata = test_adcast_AA_ups)

pred_com_AA <- pred_AA %>% 
  as.data.frame() %>%
  mutate(true_class = test_adcast_AA_ups$event)

colnames(pred_com_AA) <- c("Pred", "true_class")

conf_mat(pred_com_AA, truth = true_class, estimate = Pred)

```

Upscaled data before the split tend to be better than unbalanced datasets but worse than data after splitting the data. This is obvious since we did not feed our test data into the training part. We can see that our models underfit. We have to set an approach where we can compare both groups of events together. 

## **Synthetic Data Generation**
Another way to deal with an unbalanced dataset for our purpose is to use synthetics generation. Instead of replicating and adding the observations from the minority class, it overcomes imbalances by generating artificial data.
The synthetic minority oversampling technique (SMOTE) can be used for synthetic data generation. SMOTE algorithm creates artificial data based on feature space (rather than data space) similarities from minority samples. It generates a random set of minority class observations to shift the classifier's learning bias towards the minority class.

To generate artificial data, it uses bootstrapping and k-nearest neighbors. Precisely, it works this way:

- Take the difference between the feature vector (sample) under consideration and its nearest neighbor.
- Multiply this difference by a random number between 0 and 1
- Add it to the feature vector under consideration
- This causes the selection of a random point along the line segment between two specific features

Thanks to this method, we can generate data with similar statistical distribution along the two classes and make our model more robust toward feature data.

### **plain Synthetic data**
```{r}
### datasets synthetics adcast_A
adcast_synt_A <- ROSE(formula = event ~ ., data = subset(adcast_A, select = -Date), p = 0.4, seed = 111)$data
head(adcast_synt_A) 

table(adcast_synt_A$event)
# put ggbiplot into Console
ggbiplot(prcomp(adcast_synt_A[, 2:ncol(adcast_synt_A)], scale = TRUE, center = TRUE), 
         ellipse = TRUE,
         groups = adcast_synt_A$event, 
         alpha = 0.1, 
         varname.size = 5, 
         labels.size = 4, 
         varname.adjust = 2, 
         varname.abbrev = FALSE
         )

set.seed(1234)
split_node_syn_A <- initial_split(adcast_synt_A, prop = 0.75, strata = event)
train_A_synt <- training(split_node_syn_A)
test_A_synt <- testing(split_node_syn_A)


### datasets synthetics adcast_C
adcast_synt_C <- ROSE(formula = event ~ ., data = subset(adcast_C, select = -Date), p = 0.3, seed = 111)$data
head(adcast_synt_C)

table(adcast_synt_C$event)
# put ggbiplot, ggplotly into Console
ggbiplot(prcomp(adcast_synt_C[, 2:ncol(adcast_synt_C)], scale = TRUE, center = TRUE), 
         ellipse = TRUE,
         groups = adcast_synt_C$event, 
         alpha = 0.1, 
         varname.size = 5, 
         labels.size = 4, 
         varname.adjust = 2, 
         varname.abbrev = FALSE
         )

ggplotly(ggbiplot(prcomp(adcast_synt_C[, 2:ncol(adcast_synt_C)], scale = TRUE, center = TRUE), 
                   ellipse = TRUE,
                   groups = adcast_synt_C$event, 
                   alpha = 0.1, 
                   varname.size = 5, 
                   labels.size = 4, 
                   varname.adjust = 2, 
                   varname.abbrev = FALSE
                    )
        )

set.seed(1234)
split_node_syn_C <- initial_split(adcast_synt_C, prop = 0.75, strata = event)
train_C_synt <- training(split_node_syn_C)
test_C_synt <- testing(split_node_syn_C)



### both synthetics avalanche types
both_synth <- rbind(adcast_synt_A %>%
                    filter(event == 1) %>%
                    mutate(Type = "slab"),
                  adcast_synt_C%>%
                    filter(event == 1) %>%
                    mutate(Type = "wet")
                  )
#syntetic data PCA wet and slab
ggbiplot(prcomp(both_synth[, 2:ncol(subset(both_synth, select = - Type))], scale = TRUE, center = TRUE), 
         ellipse = TRUE,
         groups = both_synth$Type, 
         alpha = 0.2, 
         varname.size = 5, 
         labels.size = 4, 
         varname.adjust = 2, 
         varname.abbrev = FALSE
          )

ggplotly(ggbiplot(prcomp(both_synth[, 2:ncol(subset(both_synth, select = - Type))], scale = TRUE, center = TRUE), 
                 ellipse = TRUE,
                 groups = both_synth$Type, 
                 alpha = 0.2, 
                 varname.size = 5, 
                 labels.size = 4, 
                 varname.adjust = 2, 
                 varname.abbrev = FALSE
                  )
          )

## DT with synthetics data
DT_syn_A <- dt_all(frame_dt, train_A_synt, test_A_synt)
DT_syn_A

DT_syn_C <- dt_all(frame_dt, train_C_synt, test_C_synt)
DT_syn_C
```

Synthetics data can be used to evaluate the DT tree, making this technique an optimal solution for using DT. The predictability of DT is very high, and we can use DT for our data with the power of DT to explain the DT tree process.

```{r}
randomForest(formula = event ~ ., data = adcast_synt_A)
randomForest(formula = event ~ ., data = adcast_synt_C)
```

RF classifier is worse than with upscaled dataset but since we are more interested in the Variable importance plots (VIP). We can continue to train the algorithm. The DT tree will no longer be trained.


### **Synthetic data closer kernel generation**
Because our synthetic data have higher dispersion, we can generate data that can be closer to our observations. This process can lead to more accurate results since we generate observations that are "closer" to each other. We adjust the natural neighbor algorithm for data generation to generate denser observations. We have added limitations on the variables that could be generated wrongly due to physics. 

```{r}
### datasets synthetics adcast_A
adcast_synt_AA <- ROSE(formula = event ~ ., data = subset(adcast_A, select = -Date), 
                       p = 0.3, seed = 111, 
                       hmult.mino = 0.2,
                       hmult.majo = 0.2)$data
head(adcast_synt_AA) 

table(adcast_synt_AA$event)

ggbiplot(
  prcomp(adcast_synt_AA[, 2:ncol(adcast_synt_AA)], scale = TRUE, center = TRUE), 
         ellipse = TRUE,
         groups = adcast_synt_AA$event, 
         alpha = 0.1, 
         varname.size = 5, 
         labels.size = 4, 
         varname.adjust = 2, 
         varname.abbrev = FALSE
         )


## erase of the negative values 
adcast_synt_AA_t <- adcast_synt_AA %>%
  mutate(NSS_value = abs(NSS_value),
         NSS_value3 = abs(NSS_value3),
         P_value = abs(P_value),
         P_value3 = abs(P_value3),
         Rain_Ta_value = abs(Rain_Ta_value),
         Rain_Ta_value3 = abs(Rain_Ta_value3),
         Rain_Tw_value = abs(Rain_Tw_value),
         Rain_Tw_value3 = abs(Rain_Tw_value3),
         SD_value = abs(SD_value),
         SD_value3 = abs(SD_value3),
         SLd_value = abs(SLd_value),
         SLd_value3 = abs(SLd_value3),
         #Tair_value = abs(Tair_value),
         #Tair_value3 = abs(Tair_value3),
         WSavg_value = abs(WSavg_value),
         WSavg_value3 = abs(WSavg_value3),
         NSSsum3 = abs(NSSsum3),
         Rain_Ta_sum3 = abs(Rain_Ta_sum3)
         )
  
    



ggbiplot(
  prcomp(adcast_synt_AA_t[, 2:ncol(adcast_synt_AA_t)], scale = TRUE, center = TRUE), 
         ellipse = TRUE,
         groups = adcast_synt_AA_t$event, 
         alpha = 0.1, 
         varname.size = 5, 
         labels.size = 4, 
         varname.adjust = 2, 
         varname.abbrev = FALSE
         )           


##split the data
set.seed(1234)
split_node_syn_AA <- initial_split(adcast_synt_AA, prop = 0.75, strata = event)
train_AA_synt <- training(split_node_syn_AA)
test_AA_synt <- testing(split_node_syn_AA)



set.seed(1234)
split_node_syn_AA_t <- initial_split(adcast_synt_AA_t, prop = 0.75, strata = event)
train_AA_synt_t <- training(split_node_syn_AA_t)
test_AA_synt_t <- testing(split_node_syn_AA_t)


### datasets synthetics adcast_C
adcast_synt_CC <- ROSE(formula = event ~ ., data = subset(adcast_C, select = -Date), 
                       p = 0.2, 
                       seed = 111, 
                       hmult.mino = 0.2,
                       hmult.majo = 0.2)$data
head(adcast_synt_CC)

table(adcast_synt_CC$event)


## erase of the negative values 
adcast_synt_CC_t <- adcast_synt_CC %>%
  mutate(NSS_value = abs(NSS_value),
         NSS_value3 = abs(NSS_value3),
         P_value = abs(P_value),
         P_value3 = abs(P_value3),
         Rain_Ta_value = abs(Rain_Ta_value),
         Rain_Ta_value3 = abs(Rain_Ta_value3),
         Rain_Tw_value = abs(Rain_Tw_value),
         Rain_Tw_value3 = abs(Rain_Tw_value3),
         SD_value = abs(SD_value),
         SD_value3 = abs(SD_value3),
         SLd_value = abs(SLd_value),
         SLd_value3 = abs(SLd_value3),
         #Tair_value = abs(Tair_value),
         #Tair_value3 = abs(Tair_value3),
         WSavg_value = abs(WSavg_value),
         WSavg_value3 = abs(WSavg_value3),
         NSSsum3 = abs(NSSsum3),
         Rain_Ta_sum3 = abs(Rain_Ta_sum3)
         )
  
set.seed(1234)
split_node_syn_CC_t <- initial_split(adcast_synt_CC_t, prop = 0.75, strata = event)
train_CC_synt_t <- training(split_node_syn_CC_t)
test_CC_synt_t <- testing(split_node_syn_CC_t)



ggbiplot(
  prcomp(adcast_synt_CC_t[, 2:ncol(adcast_synt_CC_t)], scale = TRUE, center = TRUE), 
         ellipse = TRUE,
         groups = adcast_synt_CC_t$event, 
         alpha = 0.1, 
         varname.size = 5, 
         labels.size = 4, 
         varname.adjust = 2, 
         varname.abbrev = FALSE
         ) 

ggplotly(ggbiplot(prcomp(adcast_synt_C[, 2:ncol(adcast_synt_C)], scale = TRUE, center = TRUE), 
                   ellipse = TRUE,
                   groups = adcast_synt_C$event, 
                   alpha = 0.1, 
                   varname.size = 5, 
                   labels.size = 4, 
                   varname.adjust = 2, 
                   varname.abbrev = FALSE
                    )
        )

set.seed(1234)
split_node_syn_CC <- initial_split(adcast_synt_CC, prop = 0.75, strata = event)
train_CC_synt <- training(split_node_syn_CC)
test_CC_synt <- testing(split_node_syn_CC)



### both synthetics avalanche types
both_synth_kernel <- rbind(adcast_synt_AA %>%
                    filter(event == 1) %>%
                    mutate(Type = "slab"),
                  adcast_synt_CC%>%
                    filter(event == 1) %>%
                    mutate(Type = "wet")
                  )

ggbiplot(prcomp(both_synth_kernel[, 2:ncol(subset(both_synth_kernel, select = - Type))], scale = TRUE, center = TRUE), 
         ellipse = TRUE,
         groups = both_synth_kernel$Type, 
         alpha = 0.2, 
         varname.size = 5, 
         labels.size = 4, 
         varname.adjust = 2, 
         varname.abbrev = FALSE
          )


## DT with synthetics data
DT_syn_AA <- dt_all(frame_dt, train_AA_synt, test_AA_synt)
DT_syn_AA


DT_syn_AA_t <- dt_all(frame_dt, train_AA_synt_t, test_AA_synt_t)
DT_syn_AA_t




DT_syn_CC <- dt_all(frame_dt, train_CC_synt, test_CC_synt)
DT_syn_CC



```



```{r}
#PCA for article adcast A
## original
ggbiplot(prcomp(adcast_A[, 3:ncol(adcast_A)] %>%  select(!matches("3") & !c("Rain_Tw_value", "SDdif4", "WSavg_value", "WD_value")), scale = TRUE, center = TRUE), 
                   ellipse = TRUE, 
                   groups = adcast_A$event, 
                   alpha = 0.2, 
                   varname.size = 6, 
                   labels.size = 5, 
                   varname.adjust = 1.5, 
                   varname.abbrev = FALSE
                  ) +
  scale_x_continuous(limits = c(-4.5, 4.5)) + 
  scale_y_continuous(limits = c(-3.5, 8)) 

## smote
ggbiplot(
  prcomp(adcast_synt_AA[, 2:ncol(adcast_synt_AA)] %>%  select(!matches("3") & !c("Rain_Tw_value", "SDdif4", "WSavg_value", "WD_value")), scale = TRUE, center = TRUE), 
         ellipse = TRUE,
         groups = adcast_synt_AA$event, 
         alpha = 0.2, 
         varname.size = 6, 
         labels.size = 5, 
         varname.adjust = 1.5, 
         varname.abbrev = FALSE
         ) +
  scale_x_continuous(limits = c(-4.5, 4.5)) + 
  scale_y_continuous(limits = c(-3.5, 8))



Pca_synt_AA <- prcomp(adcast_synt_AA[, 2:ncol(adcast_synt_AA)] %>%  select(!matches("3") & !c("Rain_Tw_value", "SDdif4", "WSavg_value", "WD_value")), scale = TRUE, center = TRUE)

scree_synt_AA <- Pca_synt_AA$sdev^2 / sum(Pca_synt_AA$sdev^2)

plot(scree_synt_AA, xlab = "Principal Component",
     ylab = "proportion of variance",
     ylim = c(0, 1), type = "b")

#PCA for article adcast C
## original
ggbiplot(prcomp(adcast_C[, 3:ncol(adcast_C)] %>%  select(!matches("3") & !c("Rain_Tw_value", "SDdif4", "WSavg_value", "WD_value")), scale = TRUE, center = TRUE), 
                   ellipse = TRUE, 
                   groups = adcast_C$event, 
                   alpha = 0.2, 
                   varname.size = 6, 
                   labels.size = 5, 
                   varname.adjust = 1.5, 
                   varname.abbrev = FALSE
                  ) +
  scale_x_continuous(limits = c(-3.25, 3.25)) + 
  scale_y_continuous(limits = c(-5, 3))



## smote
ggbiplot(
  prcomp(adcast_synt_CC[, 2:ncol(adcast_synt_CC)] %>%  select(!matches("3") & !c("Rain_Tw_value", "SDdif4", "WSavg_value", "WD_value")), scale = TRUE, center = TRUE), 
         ellipse = TRUE,
         groups = adcast_synt_CC$event, 
         alpha = 0.2, 
         varname.size = 6, 
         labels.size = 5, 
         varname.adjust = 1.5, 
         varname.abbrev = FALSE
         ) +
  scale_x_continuous(limits = c(-3.25, 3.25)) + 
  scale_y_continuous(limits = c(-5, 3))


# PCA both aval types together 
## original 
both_synth_t <- rbind(adcast_synt_AA_t %>%
                    filter(event == 1) %>%
                    mutate(Type = "slab"),
                  adcast_synt_CC_t %>%
                    filter(event == 1) %>%
                    mutate(Type = "wet")
                  )

ggbiplot(prcomp(both_synth_t[, 2:ncol(both_synth_t)] %>%  select(!matches("3") & !c("Rain_Tw_value", "SDdif4", "WSavg_value", "WD_value", "Type")), scale = TRUE, center = TRUE), 
         ellipse = TRUE,
         groups = both_synth_t$Type, 
         alpha = 0.2, 
         varname.size = 5, 
         labels.size = 4, 
         varname.adjust = 2, 
         varname.abbrev = FALSE
          )



## Smote
both_synth_t <- rbind(adcast_synt_AA_t %>%
                    filter(event == 1) %>%
                    mutate(Type = "Slab"),
                  adcast_synt_CC_t %>%
                    filter(event == 1) %>%
                    mutate(Type = "Wet")
                  )

both_PCA_t <- prcomp(both_synth_t[, 2:ncol(both_synth_t)] %>%  select(!matches("3") & !c("Rain_Tw_value", "SDdif4", "WSavg_value", "WD_value", "Type")), scale = TRUE, center = TRUE)


scree_both_PCA_t <- both_PCA_t$sdev^2 / sum(both_PCA_t$sdev^2)

plot(scree_both_PCA_t, xlab = "Principal Component",
     ylab = "proportion of variance",
     ylim = c(0, 1), type = "b")

#pairslot
pairsplot(pc_b_t) + 
  ggtitle("SSSS")

pc_b_t <- pca(both_synth_t[, 2:ncol(both_synth_t)] %>%  select(!matches("3") & !c("Rain_Tw_value", "SDdif4", "WSavg_value", "WD_value", "Type")), scale = TRUE, center = TRUE)

```

Synthetics data can be used to evaluate the DT tree, making this technique an optimal solution for using DT. The predictability of DT is very high, and we can use DT for our data with the power of DT to explain the DT tree process.
We can see that synthetic data closer to the kernel are much more predictable. Our models increased in accuracy.

```{r}
randomForest(formula = event ~ ., data = adcast_synt_AA)
randomForest(formula = event ~ ., data = adcast_synt_CC)
```
Due to promising results we will implement synthetic data closer to the kerner (datasets with AA and CC suffixes) for the following comparing and modeling sections.


## **statistical distribution to synthetics data**
In this section we compare distributions from synthetics data with the distribution from the input datasets

### **adcast_A**
```{r}
#### synthetics PCA
ggbiplot(prcomp(adcast_synt_AA[, 2:ncol(adcast_synt_AA)], scale = TRUE, center = TRUE), 
         ellipse = TRUE,
         groups = adcast_synt_AA$event, 
         alpha = 0.1, 
         varname.size = 5, 
         labels.size = 4, 
         varname.adjust = 2, 
         varname.abbrev = FALSE
         )

#### adcast_A PCA
ggbiplot(prcomp(adcast_A[, 3:ncol(adcast_A)], scale = TRUE, center = TRUE), 
         ellipse = TRUE, 
         groups = adcast_A$event, 
         alpha = 0.2, 
         varname.size = 5, 
         labels.size = 4, 
         varname.adjust = 2, 
         varname.abbrev = TRUE
         ) 

#### combining datasets
qq_comp_A <- rbind(subset(adcast_A, select = - Date) %>%
                     mutate(Type = "A"), 
                   adcast_synt_AA %>%
                     mutate(Type = "Synt_A"))

#### base metrics for randomly picked variables
qq_comp_A %>%
  group_by(Type, event) %>%
  dplyr::summarize(H_value_mean = mean(H_value),
                   H_value_median = median(H_value),
                   NSS_value_mean = mean(NSS_value),
                   NSS_value_median = median(NSS_value),
                   SD_value_mean = mean(SD_value),
                   SD_value_median = median(SD_value))

qq_comp_A <- qq_comp_A %>%
  select(!matches("3"))

qq_comp_A_t <- qq_comp_A %>%
  pivot_longer(cols = c("H_value", "NSS_value", "P_value", "Rain_Ta_value", "Rain_Tw_value", "SD_value", "SLd_value", "Tair_value", "WD_value", "WSavg_value", "SDdif2", "SDdif4"), names_to = "variable", values_to = "value")


#### Q-Q plot comparison imput, synthetic
ggplot(qq_comp_A_t, aes(sample = value, col = Type)) + 
  geom_qq() + 
  xlab("Theoretical") +
  ylab("Sample") + 
  facet_wrap(~ variable, scales = "free")

#### boxplots 
ggplot(qq_comp_A_t, aes(value, fill = Type)) + 
  geom_boxplot() + 
  facet_wrap(~ variable, scales = "free")

```


### **adcast_C**
```{r}
#### synthetics PCA
ggbiplot(prcomp(adcast_synt_CC[, 2:ncol(adcast_synt_CC)], scale = TRUE, center = TRUE), 
         ellipse = TRUE,
         groups = adcast_synt_CC$event, 
         alpha = 0.1, 
         varname.size = 5, 
         labels.size = 4, 
         varname.adjust = 2, 
         varname.abbrev = FALSE
         )

#### adcast_C PCA
ggbiplot(prcomp(adcast_C[, 3:ncol(adcast_C)], scale = TRUE, center = TRUE), 
         ellipse = TRUE, 
         groups = adcast_C$event, 
         alpha = 0.2, 
         varname.size = 5, 
         labels.size = 4, 
         varname.adjust = 2, 
         varname.abbrev = TRUE
         ) 

#### combining datasets
qq_comp_C <- rbind(subset(adcast_C, select = - Date) %>%
                     mutate(Type = "C"), 
                   adcast_synt_CC %>%
                     mutate(Type = "Synt_C"))

#### base metrics for randomly picked variables
qq_comp_C %>%
  group_by(Type, event) %>%
  dplyr::summarize(H_value_mean = mean(H_value),
                   H_value_median = median(H_value),
                   NSS_value_mean = mean(NSS_value),
                   NSS_value_median = median(NSS_value),
                   SD_value_mean = mean(SD_value),
                   SD_value_median = median(SD_value))

qq_comp_C <- qq_comp_C %>%
  select(!matches("3"))

qq_comp_C_t <- qq_comp_C %>%
  pivot_longer(cols = c("H_value", "NSS_value", "P_value", "Rain_Ta_value", "Rain_Tw_value", "SD_value", "SLd_value", "Tair_value", "WD_value", "WSavg_value", "SDdif2", "SDdif4"), names_to = "variable", values_to = "value")


#### Q-Q plot
ggplot(qq_comp_C_t, aes(sample = value, col = Type)) + 
  geom_qq() + 
  xlab("Theoretical") +
  ylab("Sample") + 
  facet_wrap(~ variable, scales = "free")


#### boxplots 
ggplot(qq_comp_C_t, aes(value, fill = Type)) + 
  geom_boxplot() + 
  facet_wrap(~ variable, scales = "free")

```
Synthetic data follow statistical distribution of the input datasets. Some distortion can be found in the Q-Q plots but the most important and significant parts overlap. 

# **Supervised Learning & final modelling**
```{r}
## RF classifier
### iterace RF adcast_synt_C & A
RF_it <- data.frame(model = letters[seq(1,7,1)],
                    ntree = c(250, 500, 800, 1000, 1500, 2000, 6000))

### 
models <- list()
max_oobs <- c()
var_imp <- list()
pred_comb <- list()
auc_models <- list()

### Iterace 
for(i in 1:nrow(RF_it)) {
  set.seed(2)
  models[[i]] <- randomForest(formula = event ~ ., 
                              data = train_AA_synt,
                              ntree = RF_it$ntree[i])
  
  # print(models[[i]]) -- models information
  plot(models[[i]], main = paste("model", RF_it$model[i], sep = "_"))
  legend(x = "right", 
         legend = colnames(models[[i]]$err.rate),
         fill = 1:ncol(models[[i]]$err.rate))
  
  varImpPlot(models[[i]], main = paste("model", RF_it$model[i], sep = "_"))   
  var_imp[[RF_it$model[i]]] <- models[[i]]$importance
  max_oobs[i] <- models[[i]]$err.rate[nrow(models[[i]]$err.rate), "OOB"]
  
  pred_comb[[i]] <- predict(object = models[[i]],
                            newdata = test_AA_synt,
                            type = "prob") %>%
    as.data.frame() %>%
    mutate(true_class = test_AA_synt$event)
  
  auc_models[[RF_it$model[i]]] <- roc_auc(pred_comb[[i]], 
                                          estimate = 1,
                                          truth = true_class)
}


### iteration results dataset_d
max_oobs
var_imp
auc_models
```

First, we iterate through several trees to see when the OOB decreases. Hence we can establish an optimal number of trees used for RF. In our case, the number of trees did not affect the OBB, so we can use a smaller number of trees for our model since the default number of trees in RF equals 500. We used 500 trees.

```{r}
### Hyperparameters RF 

rf_all <- function(trainset, testset) {
  
  #### Definition of parametres
  mtry <- seq(2, ncol(trainset) * 0.8, 2)
  nodesize <- seq(1, 10, 1)
  sampsize <- round(nrow(trainset) * c(0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9))
  
  #### Grid definition
  hyper_grid <- expand.grid(mtry = mtry, nodesize = nodesize, sampsize = sampsize)
  oob_err <- c()
  
  #### Grid iteration
  for (i in 1:nrow(hyper_grid)) {
    model_loop <- randomForest(formula = event ~ ., 
                               data = trainset,
                               ntree = 500,
                               strata = event,
                               mtry = hyper_grid$mtry[i],
                               nodesize = hyper_grid$nodesize[i],
                               sampsize = hyper_grid$sampsize[i])
    
    oob_err[i] <- model_loop$err.rate[nrow(model_loop$err.rate), "OOB"]
  }
  
  opt_i <- which.min(oob_err)
  
  #### Parameters after iteration
  best_parameters <- hyper_grid[opt_i, ]
  
  #### Model fit 
  mod_par <- randomForest(formula = event ~ .,
                            data = trainset,
                            ntree = 500,
                            mtry = best_parameters$mtry,
                            nodesize = best_parameters$nodesize,
                            sampsize =  best_parameters$sampsize
                            )
  
  pred <- predict(object = mod_par,
                  newdata = testset,
                  type = "prob")
  
  pred_com_rf <- pred %>% 
    as.data.frame() %>%
    mutate(true_class = testset$event)
  
  roc_rf <- roc_curve(pred_com_rf,
                         estimate = 1,
                         truth = true_class) %>% autoplot()
  
  auc_rf <- roc_auc(pred_com_rf,
                       estimate = 1,
                       truth = true_class)
  
  all_objects <- list(hyper_grid, oob_err, best_parameters, roc_rf, auc_rf)
  return(all_objects)
}


#### RF tuning 
rf_A_synt <- rf_all(train_AA_synt_t, test_AA_synt_t)
rf_C_synt <- rf_all(train_CC_synt_t, test_CC_synt_t)


##### tuning graphs
dt_obb_A <- cbind(oob = rf_A_synt[[2]], rf_A_synt[[1]])
dt_obb_C <- cbind(oob = rf_C_synt[[2]], rf_C_synt[[1]])

ggplot(dt_obb_A, aes(x = oob)) + 
  geom_bar()

ggplot(dt_obb_C, aes(x = oob)) + 
  geom_bar()

ggplot(dt_obb_A, aes(x = oob, y = mtry, col = nodesize, size = sampsize)) + 
  geom_point()

ggplot(dt_obb_C, aes(x = oob, y = mtry, col = nodesize, size = sampsize)) + 
  geom_point()

ggplot(dt_obb_A, aes(x = mtry, y = oob, col = nodesize)) + 
  geom_path()

ggplot(dt_obb_C, aes(x = mtry, y = oob, col = nodesize)) + 
  geom_path()
```

We trained our model to obtain the best hyperparameters for our predictions. The Hyperparameters did not cause significant improvement. But we can be sure that our model is reliable and robust. To improve our model we could use cross validation (CV) but it is not necessary.

Now we can use our parameters to fit the model again.

```{r}
### final model A
adcast_A_model <- randomForest(formula = event ~ .,
                               data = train_AA_synt_t,
                               importance = TRUE,
                               mtry = rf_A_synt[[3]]$mtry,
                               nodesize = rf_A_synt[[3]]$nodesize,
                               sampsize = rf_A_synt[[3]]$sampsize)

pred_model_A <- predict(object = adcast_A_model,
                newdata = test_AA_synt_t,
                type = "prob")

pred_com_rf_A <- pred_model_A %>% 
  as.data.frame() %>%
  mutate(true_class = test_AA_synt_t$event)

roc_model_A <- roc_curve(pred_com_rf_A,
                        estimate = 1,
                        truth = true_class) %>% autoplot() + ggtitle("Roc curve slab")

auc_rf_A <- roc_auc(pred_com_rf_A,
                  estimate = 1,
                  truth = true_class)


#### RF model A all bootstrap
model_A_ALL <- randomForest(formula = event ~ .,
                            data = adcast_synt_AA_t,
                            importance = TRUE,
                            mtry = rf_A_synt[[3]]$mtry,
                            nodesize = rf_A_synt[[3]]$nodesize,
                            sampsize = rf_A_synt[[3]]$sampsize)


### final model C
adcast_C_model <- randomForest(formula = event ~ .,
                               data = train_CC_synt_t,
                               importance = TRUE,
                               mtry = rf_C_synt[[3]]$mtry,
                               nodesize = rf_C_synt[[3]]$nodesize,
                               sampsize = rf_C_synt[[3]]$sampsize)

pred_model_C <- predict(object = adcast_C_model,
                        newdata = test_CC_synt_t,
                        type = "prob")

pred_com_rf_C <- pred_model_C %>% 
  as.data.frame() %>%
  mutate(true_class = test_CC_synt_t$event)

roc_model_C <- roc_curve(pred_com_rf_C,
                         estimate = 1,
                         truth = true_class) %>% autoplot() + ggtitle("Roc curve wet")

auc_rf_C <- roc_auc(pred_com_rf_C,
                    estimate = 1,
                    truth = true_class)


#### RF model C all bootstrap
model_C_ALL <- randomForest(formula = event ~ .,
                            data = adcast_synt_CC_t,
                            importance = TRUE,
                            mtry = rf_C_synt[[3]]$mtry,
                            nodesize = rf_C_synt[[3]]$nodesize,
                            sampsize = rf_C_synt[[3]]$sampsize)
```

As we train RF, we can use it for the evaluation train/test set. However, we can implement the whole dataset for the final model due to RF bootstrapping. Therefore we ended up with two models for each dataset. Again, CV would be beneficial. 

## **VIP**

```{r}
## VIP

varImpPlot(adcast_A_model)
varImpPlot(model_A_ALL)

varImpPlot(adcast_C_model)
varImpPlot(model_C_ALL)


vip(adcast_A_model$importance,  num_features = 30)
vip(model_A_ALL, num_features = 30)

vip(adcast_C_model,  num_features = 30)
vip(model_C_ALL, num_features = 30)


```

VIP from both models per dataset. 
```{r}
## VIP mean decrease accuracy
importance_A <- importance(model_A_ALL)
varImportance_A <- data.frame(Variables = row.names(importance_A),
Importance_A <- round(importance_A[,"MeanDecreaseAccuracy"],2))


rankImportance_A <- varImportance_A %>% 
  mutate(Rank = paste("#", dense_rank(desc(Importance_A))))
                                                     
ggplot(rankImportance_A, aes(x = reorder(Variables, Importance_A), y = Importance_A, fill = Importance_A))+ 
 geom_bar(stat = "identity") + 
 geom_text(aes(x = Variables, y = 0.5, label = Rank),
           hjust = 0, 
           vjust = 0.55, 
           size = 4, 
           colour = "white") +
           labs(x = "Variables", y = "Importance") +
 coord_flip() +
  ggtitle("Variable importance for slab dataset") +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), legend.position = "none")



importance_C <- importance(model_C_ALL)
varImportance_C <- data.frame(Variables = row.names(importance_C),
Importance_C <- round(importance_C[,"MeanDecreaseAccuracy"],2))


rankImportance_C <- varImportance_C %>% 
  mutate(Rank = paste("#", dense_rank(desc(Importance_C))))
                                                     
ggplot(rankImportance_C, aes(x = reorder(Variables, Importance_C), y = Importance_C, fill = Importance_C))+ 
 geom_bar(stat = "identity") + 
 geom_text(aes(x = Variables, y = 0.5, label = Rank),
           hjust = 0, 
           vjust = 0.55, 
           size = 4, 
           colour = "white") +
           labs(x = "Variables", y = "Importance") +
 coord_flip() +
  ggtitle("Variable importance for wet dataset") +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), legend.position = "none")

```
## **predict on input data**
```{r}
### model fit on input data adcast_A with adcast_A_model
in_pred_A <- predict(object = adcast_A_model, newdata = adcast_A)

in_pred_com_A <- in_pred_A %>% 
  as.data.frame() %>%
  mutate(true_class = adcast_A$event)

colnames(in_pred_com_A) <- c("Pred", "true_class")

conf_mat(in_pred_com_A, truth = true_class, estimate = Pred)


### model fit on input data adcast_A with model_A_ALL
in_pred_AA <- predict(object = model_A_ALL, newdata = adcast_A)

in_pred_com_AA <- in_pred_AA %>% 
  as.data.frame() %>%
  mutate(true_class = adcast_A$event)

colnames(in_pred_com_AA) <- c("Pred", "true_class")

conf_mat(in_pred_com_AA, truth = true_class, estimate = Pred)


### model fit on input data adcast_C with adcast_C_model
in_pred_C <- predict(object = adcast_C_model, newdata = adcast_C)

in_pred_com_C <- in_pred_C %>% 
  as.data.frame() %>%
  mutate(true_class = adcast_C$event)

colnames(in_pred_com_C) <- c("Pred", "true_class")

conf_mat(in_pred_com_C, truth = true_class, estimate = Pred)


### model fit on input data adcast_C with model_C_ALL
in_pred_CC <- predict(object = model_C_ALL, newdata = adcast_C)

in_pred_com_CC <- in_pred_CC %>% 
  as.data.frame() %>%
  mutate(true_class = adcast_C$event)

colnames(in_pred_com_CC) <- c("Pred", "true_class")

conf_mat(in_pred_com_CC, truth = true_class, estimate = Pred)


```
Our models predict avalanche events well but tend to predict events that were not marked as avalanche bad. In the real scenario, we would falsely inform that there is a high probability of avalanche events. This false information can be thought of as a warning for future modeling. To get more accurate results, we would need to analyse data deeper. 



```{r}
#Final outputs
#DT upscale





```